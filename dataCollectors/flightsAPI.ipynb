{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MINzv4e82A0k",
        "outputId": "44868216-edf0-4b89-80c2-8f9920a2073e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=ef352baeb64d7391d4d18a7b5da190eb2b81cfa75c29a2c9ee0653cf6b65b418\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_P5gZrlolkb",
        "outputId": "46101d04-2fd1-412b-d613-33a8cc6b3d86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting FlightRadarAPI\n",
            "  Downloading flightradarapi-1.3.26-py3-none-any.whl (16 kB)\n",
            "Collecting brotli (from FlightRadarAPI)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from FlightRadarAPI) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->FlightRadarAPI) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->FlightRadarAPI) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->FlightRadarAPI) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->FlightRadarAPI) (2024.2.2)\n",
            "Installing collected packages: brotli, FlightRadarAPI\n",
            "Successfully installed FlightRadarAPI-1.3.26 brotli-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install FlightRadarAPI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iy4YNvIa1n8l"
      },
      "outputs": [],
      "source": [
        "from FlightRadar24 import FlightRadar24API\n",
        "import json\n",
        "import re\n",
        "fr_api = FlightRadar24API()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_AWAJgmV1vi2"
      },
      "outputs": [],
      "source": [
        "def save_json(dicc,file):\n",
        "    with open(file,'w') as f:\n",
        "        f.write(json.dumps(dicc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qusGIFMF1x1P"
      },
      "outputs": [],
      "source": [
        "def read_json(file):\n",
        "    with open(file,'r') as f:\n",
        "        data = json.load(f)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDb0ebkCoOUs"
      },
      "source": [
        "## Obtain airports from cities we are interested in\\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ir21lJck1zVG"
      },
      "outputs": [],
      "source": [
        "airports = fr_api.get_airports()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "i91hdRfuoUY0"
      },
      "outputs": [],
      "source": [
        "# List of cities\n",
        "cities = ['Amsterdam','Athens','Barcelona','Berlin','Budapest','Lisbon','London','Paris','Rome','Vienna']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TrN_W97SovkZ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "regex = r'\\((.*?)\\)'\n",
        "airports_in_city = []\n",
        "\n",
        "for ap in airports:\n",
        "    for city in cities:\n",
        "        if city in str(ap):\n",
        "            acronym = re.findall(regex,str(ap))[0]\n",
        "            airports_in_city.append(ap)\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RQvEcmJxoxk7"
      },
      "outputs": [],
      "source": [
        "def parseig_aeroports(lista_aeropuertos):\n",
        "    airports_info = {}\n",
        "    regex = r'\\((.*?)\\) .*? Latitude: ([-+]?\\d+\\.\\d+) - Longitude: ([-+]?\\d+\\.\\d+)'\n",
        "\n",
        "    for aeropuerto in lista_aeropuertos:\n",
        "        matches = re.findall(regex, str(aeropuerto))\n",
        "        if matches:\n",
        "            siglas = matches[0][0]\n",
        "            latitude = float(matches[0][1])\n",
        "            longitude = float(matches[0][2])\n",
        "            for city in cities:\n",
        "                if city in aeropuerto:\n",
        "                    airp_city = city\n",
        "            airports_info[siglas] = {'Latitude': latitude, 'Longitude': longitude,'City':airp_city}\n",
        "\n",
        "    return airports_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "cL2kz2Raoy-B"
      },
      "outputs": [],
      "source": [
        "airports_info = parseig_aeroports(airports_in_city)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJG4U_RRo3d9"
      },
      "source": [
        "We store information we have by the moment because we had some issues with API requests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3Ya2Mjho5Lv"
      },
      "outputs": [],
      "source": [
        "save_json(airports_info,'airports_info.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO_QV21Po7BW"
      },
      "source": [
        "## Get arrivals from airports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_mlE7mOo8WI"
      },
      "outputs": [],
      "source": [
        "airports_info = read_json('airports_info.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeHhmJzko9f_"
      },
      "outputs": [],
      "source": [
        "def get_100_arrivals(acronym):\n",
        "    aeroport = fr_api.get_airport_details(acronym)\n",
        "    vols = aeroport['airport']['pluginData']['schedule']['arrivals']['data']\n",
        "    return vols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhdqEYP8o-1u"
      },
      "outputs": [],
      "source": [
        "for airp in airports_info.keys():\n",
        "    airports_info[airp]['arrivals'] = get_100_arrivals(airp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fdbmva_So_-g"
      },
      "outputs": [],
      "source": [
        "save_json(airports_info,'airport_info_arrivals.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aCB69lYpB8Q"
      },
      "source": [
        "## Save into DataLake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rTrvGcZpDKl"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NU7-PgG-pYos"
      },
      "outputs": [],
      "source": [
        "airports_info = read_json('airport_info_arrivals.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Luqz-y0FpMOg"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Init session in Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"JSON a Parquet con Spark SQL\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Read json and create a temporal view\n",
        "df = spark.read.json(\"airport_info_arrivals.json\")\n",
        "df.createOrReplaceTempView(\"airport_info_arrivals\")\n",
        "\n",
        "# Use Spark SQL to select the data\n",
        "selected_data = spark.sql(\"SELECT * FROM airport_info_arrivals\")\n",
        "\n",
        "# Save data as a parquet file\n",
        "selected_data.write.parquet(\"datalake/airport_info_arrivals.parquet\")\n",
        "\n",
        "# Stop Spark Session\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL22QRecpRX_"
      },
      "source": [
        "We realized that out json didn't have a great format to manage the data as we want. So we need to change it to have the following structure to begin:\n",
        "\n",
        "| Airport acronym | Longitude | Latitude | City  | arrivals |\n",
        "| --------------- | --------- | -------- | ----  | -------- |\n",
        "| EHAM | 52.308609 | 4.763889 | Amsterdam  | [flight 3473cc55] |\n",
        "\n",
        "To get this new structure we'll use map reduce, as we have seen in class. Then we'll built another parquet file and it will be the definitive of our datalake."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8bSzA808XWv",
        "outputId": "993dc254-33b0-47e9-b58e-8cefa83b8b5a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Map function: transform input into intermediate key-value output\n",
        "def map_function(input_data):\n",
        "    intermediate = []\n",
        "    for key, value in input_data.items():\n",
        "        # The key is the airport acronym, and the value is the corresponding information\n",
        "        intermediate.append((key, value))\n",
        "    return intermediate\n",
        "\n",
        "# Reduce function: process intermediate values for each key and emit final result\n",
        "# In this case, each key is unique, so we just pass the data through\n",
        "def reduce_function(intermediate_data):\n",
        "    result = []\n",
        "    for key, values in intermediate_data:\n",
        "        # Transform the value to the desired structure (in this case, it's already in the desired structure)\n",
        "        result.append((key, values['Longitude'], values['Latitude'], values['City'], values['arrivals']))\n",
        "    return result\n",
        "\n",
        "# Simulate the MapReduce process\n",
        "intermediate = map_function(airports_info)\n",
        "final_result = reduce_function(intermediate)\n",
        "\n",
        "# Display the result\n",
        "for item in final_result:\n",
        "    print(item)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOQH6yjp9umF",
        "outputId": "b528f140-7c09-47a5-f525-6c7237fc825c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+----------+---------+---------+--------------------+\n",
            "|airport_acronym| longitude| latitude|     city|            arrivals|\n",
            "+---------------+----------+---------+---------+--------------------+\n",
            "|           EHAM|  4.763889|52.308609|Amsterdam|[{flight={owner={...|\n",
            "|           KAHN|-83.324722| 33.95195|   Athens|[{flight={owner=n...|\n",
            "|           LGAV|  23.94446|37.936352|   Athens|[{flight={owner={...|\n",
            "|           KUNI|  -82.2314|39.210999|   Athens|                  []|\n",
            "|           KMMI|  -84.5625|35.397221|   Athens|                  []|\n",
            "|           LEBL|  2.078463| 41.29707|Barcelona|[{flight={owner={...|\n",
            "|           SVBC|-64.689102| 10.10713|Barcelona|[{flight={owner={...|\n",
            "|           EDDB| 13.503722|52.362877|   Berlin|[{flight={owner={...|\n",
            "|           LHBS| 18.980589|47.451065| Budapest|                  []|\n",
            "|           LHBP|  19.25559|47.436932| Budapest|[{flight={owner={...|\n",
            "|           FAEL|  27.82593| -33.0355|   London|[{flight={owner={...|\n",
            "|           LPPT|  -9.13591|38.781311|   Lisbon|[{flight={owner={...|\n",
            "|           EGKB|    0.0325| 51.33083|   London|[{flight={owner={...|\n",
            "|           EGLC|  0.055278|51.505268|   London|[{flight={owner={...|\n",
            "|           KLOZ|-84.076942|37.087223|   London|                  []|\n",
            "|           EGTR| -0.325833|51.655834|   London|                  []|\n",
            "|           EGKK|  -0.19027|51.148048|   London|[{flight={owner={...|\n",
            "|           EGLL| -0.467081|51.471626|   London|[{flight={owner={...|\n",
            "|           CYXU|  -81.1511|43.033051|   London|[{flight={owner=n...|\n",
            "|           EGGW|  -0.36833|51.874722|   London|[{flight={owner={...|\n",
            "+---------------+----------+---------+---------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, DoubleType\n",
        "# Define the DataFrame schema according to the desired structure\n",
        "schema = StructType([\n",
        "    StructField(\"airport_acronym\", StringType(), True),\n",
        "    StructField(\"longitude\", DoubleType(), True),\n",
        "    StructField(\"latitude\", DoubleType(), True),\n",
        "    StructField(\"city\", StringType(), True),\n",
        "    StructField(\"arrivals\", ArrayType(StringType()), True)\n",
        "])\n",
        "\n",
        "# Start a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Load and Save Transformed Data\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create a DataFrame from the transformed data and the defined schema\n",
        "df = spark.createDataFrame(data=final_result, schema=schema)\n",
        "\n",
        "# Show the DataFrame to verify its content\n",
        "df.show()\n",
        "\n",
        "# Save the DataFrame as a Parquet file\n",
        "df.write.parquet(\"datalake/airport_info_transformed.parquet\")\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
