{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlightRadar24 import FlightRadar24API\n",
    "import json\n",
    "import re\n",
    "fr_api = FlightRadar24API()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file):\n",
    "    with open(file,'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(dicc,file):\n",
    "    with open(file,'w') as f:\n",
    "        f.write(json.dumps(dicc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain airports from cities we are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = fr_api.get_airports()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of cities\n",
    "cities = ['Amsterdam','Athens','Barcelona','Berlin','Budapest','Lisbon','London','Paris','Rome','Vienna']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regex = r'\\((.*?)\\)'\n",
    "airports_in_city = []\n",
    "\n",
    "for ap in airports:\n",
    "    for city in cities:\n",
    "        if city in str(ap):\n",
    "            acronym = re.findall(regex,str(ap))[0]\n",
    "            airports_in_city.append(ap)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_in_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseig_aeroports(lista_aeropuertos):\n",
    "    airports_info = {}\n",
    "    regex = r'\\((.*?)\\) .*? Latitude: ([-+]?\\d+\\.\\d+) - Longitude: ([-+]?\\d+\\.\\d+)'\n",
    "    \n",
    "    for aeropuerto in lista_aeropuertos:\n",
    "        matches = re.findall(regex, str(aeropuerto))\n",
    "        if matches:\n",
    "            siglas = matches[0][0]\n",
    "            latitude = float(matches[0][1])\n",
    "            longitude = float(matches[0][2])\n",
    "            for city in cities:\n",
    "                if city in aeropuerto:\n",
    "                    airp_city = city\n",
    "            airports_info[siglas] = {'Latitude': latitude, 'Longitude': longitude,'City':airp_city}\n",
    "    \n",
    "    return airports_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_info = parseig_aeroports(airports_in_city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store information we have by the moment because we had some issues with API requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(airports_info,'airports_info.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get arrivals from airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_info = read_json('airports_info.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_100_arrivals(acronym):\n",
    "    aeroport = fr_api.get_airport_details(acronym)\n",
    "    vols = aeroport['airport']['pluginData']['schedule']['arrivals']['data']\n",
    "    return vols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for airp in airports_info.keys():\n",
    "    airports_info[airp]['arrivals'] = get_100_arrivals(airp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(airports_info,'airport_info_arrivals.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save into DataLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_info = read_json('../airport_info_arrivals.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Init session in Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"JSON a Parquet con Spark SQL\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read json and create a temporal view\n",
    "df = spark.read.json(\"airport_info_arrivals.json\")\n",
    "df.createOrReplaceTempView(\"airport_info_arrivals\")\n",
    "\n",
    "# Use Spark SQL to select the data\n",
    "selected_data = spark.sql(\"SELECT * FROM airport_info_arrivals\")\n",
    "\n",
    "# Save data as a parquet file\n",
    "selected_data.write.parquet(\"../datalake/airport_info_arrivals.parquet\")\n",
    "\n",
    "# Stop Spark Session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We realized that out json didn't have a great format to manage the data as we want. So we need to change it to have the following structure to begin:\n",
    "\n",
    "| Airport acronym | Longitude | Latitude | City  | arrivals | \n",
    "| --------------- | --------- | -------- | ----  | -------- | \n",
    "| EHAM | 52.308609 | 4.763889 | Amsterdam  | [flight 3473cc55] |\n",
    "\n",
    "To get this new structure we'll use map reduce, as we have seen in class. Then we'll built another parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map function: transform input into intermediate key-value output\n",
    "def map_function(input_data):\n",
    "    intermediate = []\n",
    "    for key, value in input_data.items():\n",
    "        # The key is the airport acronym, and the value is the corresponding information\n",
    "        intermediate.append((key, value))\n",
    "    return intermediate\n",
    "\n",
    "# Reduce function: process intermediate values for each key and emit final result\n",
    "# In this case, each key is unique, so we just pass the data through\n",
    "def reduce_function(intermediate_data):\n",
    "    result = []\n",
    "    for key, values in intermediate_data:\n",
    "        # Transform the value to the desired structure (in this case, it's already in the desired structure)\n",
    "        result.append((key, values['Longitude'], values['Latitude'], values['City'], values['arrivals']))\n",
    "    return result\n",
    "\n",
    "# Simulate the MapReduce process\n",
    "intermediate = map_function(airports_info)\n",
    "final_result = reduce_function(intermediate)\n",
    "\n",
    "# Display the result\n",
    "for item in final_result:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, DoubleType\n",
    "# Define the DataFrame schema according to the desired structure\n",
    "schema = StructType([\n",
    "    StructField(\"Airport acronym\", StringType(), True),\n",
    "    StructField(\"Longitude\", DoubleType(), True),\n",
    "    StructField(\"Latitude\", DoubleType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"arrivals\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "# Start a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Load and Save Transformed Data\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a DataFrame from the transformed data and the defined schema\n",
    "df = spark.createDataFrame(data=final_result, schema=schema)\n",
    "\n",
    "# Show the DataFrame to verify its content\n",
    "df.show()\n",
    "\n",
    "# Save the DataFrame as a Parquet file\n",
    "df.write.parquet(\"../datalake/airport_info_transformed.parquet\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
